#posit.cloud 통해서 new project 생성

#성과관리 논문 read 

install.packages("openxlsx") #엑셀을 불러올 수 있는 패키지 끌어오기
library(openxlsx) #패키지 내 라이브러리 설치

sheet<-loadWorkbook("ORG.xlsx") #성과주의 논문 sheet 가져오기
data1<-read.xlsx(sheet, sheet="sheet1") #sheet1 끌어와서 데이터 형식으로 변환
View(data1) #데이터 잘 끌어왔는지 보기

##### 전처리 하기 불용어 안건드리고 단어 빈도수 보기

install.packages("stringr") #텍스트에서 문장부호나 한글에 해당하지 않는 문자 등을 지우기 위한 패키지 
library(stringr)
install.packages("dplyr") # 텍스트 처리 함수 적용을 위해 tibble 구조로 변환하기 위한 패키지 설치
library(dplyr)
install.packages("tidytext") #텍스트 정돈된 형태로 바꾸는 패키지
library(tidytext)

ORG <-data1 %>%
str_replace_all ("[^가-힣]", " ") %>% #한글 이외 모든 언어 삭제
str_squish() %>% #연속된 공백 제거 
as_tibble() #tibble로 전환 

word_ORG <-ORG %>%
unnest_tokens(input = value, output = word, token = "words") #띄어쓰기 기준으로 토큰화 

word_ORG <- word_ORG %>% 
count(word, sort = T) #빈도 수 높은 순으로 정리 

##형태소 정리하기 

install.packages("multilinguer")
library(multilinguer)
install_jdk()
install.packages(c("stringr", "hash", "tau", "Sejong", "RSQLite", "devtools"))
install.packages("remotes")
remotes::install_github("haven-jeon/KoNLP", upgrade = "never", INSTALL_opts = c("--no-multiarch"))

library(KoNLP)
useNIADic()

##명사 추출하기
library(KoNLP)
library(dplyr)
library(tidytext)

word_noun <- ORG %>%
unnest_tokens(input = value, output = word, token = extractNoun) 

##명사 빈도 분석하기 
word_noun <- word_noun %>%
count(word, sort = T) %>%
filter(str_count(word) > 1) # 두글자 이상만 남기기

#TF-IDF 구하기
sheet<-loadWorkbook("ORG.xlsx") #성과주의 논문 sheet 가져오기
data2<-read.xlsx(sheet, sheet="sheet1") #sheet1 끌어와서 데이터 형식으로 변환
View(data2) #데이터 잘 끌어왔는지 보기

TFdata <- data2 %>%
str_replace_all ("[^가-힣]", " ") %>% #한글 이외 모든 언어 삭제
str_squish() %>% #연속된 공백 제거 

TFdata<-TFdata %>%
unnest_tokens(input = value, output = word, token = extractNoun) %>%
arrange(-tf_idf)

